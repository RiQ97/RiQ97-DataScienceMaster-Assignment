{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "A1. **R-squared (R¬≤)**:\n",
    "- Represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "- Indicates how well the regression model fits the data.\n",
    "\n",
    "**Calculation**:\n",
    "$$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
    "- SS_{res}: Sum of squared residuals (difference between observed and predicted values).\n",
    "- SS_{tot}: Total sum of squares (difference between observed values and the mean of observed values).\n",
    "\n",
    "**Interpretation**:\n",
    "- Ranges from 0 to 1.\n",
    "  -  R^2 = 1 : Perfect fit, model explains all the variability.\n",
    "  -  R^2 = 0: Model explains none of the variability.\n",
    "- Higher R^2 indicates a better fit.\n",
    "\n",
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "A2. **Adjusted R-squared** adjusts the R¬≤ value for the number of predictors in the model, preventing overestimation of the goodness of fit.\n",
    "\n",
    "**Formula**:\n",
    "$$ \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right) $$\n",
    "- n: Number of observations.\n",
    "- p: Number of predictors.\n",
    "\n",
    "- Adjusted R¬≤ accounts for the number of predictors and penalizes the addition of unnecessary predictors.\n",
    "- Regular R¬≤ can increase with the addition of more predictors, even if they are irrelevant.\n",
    "\n",
    "### Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "A3. **Use Adjusted R-squared**:\n",
    "- When comparing models with different numbers of predictors.\n",
    "- To avoid overfitting and ensure the model's additional predictors are meaningful.\n",
    "- When the goal is to select the best model among several with different numbers of predictors.\n",
    "\n",
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "A4. **RMSE (Root Mean Squared Error)**:\n",
    "$$ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "- Represents the square root of the average squared differences between predicted and observed values.\n",
    "- Sensitive to outliers.\n",
    "\n",
    "**MSE (Mean Squared Error)**:\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "- Average squared differences between predicted and observed values.\n",
    "- Also sensitive to outliers.\n",
    "\n",
    "**MAE (Mean Absolute Error)**:\n",
    "$$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "- Average absolute differences between predicted and observed values.\n",
    "- Less sensitive to outliers compared to RMSE and MSE.\n",
    "\n",
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "A5. \n",
    "1. **RMSE**:\n",
    "- **Advantages**: Penalizes large errors more than smaller ones, providing a measure of model accuracy.\n",
    "- **Disadvantages**: Sensitive to outliers, can be misleading if large errors are present.\n",
    "\n",
    "2. **MSE**:\n",
    "- **Advantages**: Similar to RMSE, provides an average of squared errors, useful for optimization.\n",
    "- **Disadvantages**: Also sensitive to outliers, and not in the same units as the original data.\n",
    "\n",
    "3. **MAE**:\n",
    "- **Advantages**: Less sensitive to outliers, provides a straightforward measure of average error.\n",
    "- **Disadvantages**: May not penalize large errors enough, providing less information about model performance.\n",
    "\n",
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "A6. **Lasso Regularization**:\n",
    "- Adds a penalty equal to the absolute value of the magnitude of coefficients to the cost function.\n",
    "$$ \\text{Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\n",
    "- Can shrink some coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "**Difference from Ridge Regularization**:\n",
    "- Ridge adds a penalty equal to the square of the magnitude of coefficients.\n",
    "$$ \\text{Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2 $$\n",
    "- Ridge can only shrink coefficients but not set them to zero.\n",
    "\n",
    "**When to Use Lasso**:\n",
    "- When feature selection is desired.\n",
    "- When dealing with high-dimensional data where some predictors may be irrelevant.\n",
    "\n",
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "A7. **Regularized Linear Models**:\n",
    "- Add a penalty to the cost function, discouraging overly complex models.\n",
    "- Prevent overfitting by shrinking coefficients, reducing model complexity.\n",
    "\n",
    "**Example**:\n",
    "- Without regularization: A model with high variance fits the training data too closely.\n",
    "- With regularization (Ridge or Lasso): The model's coefficients are penalized, leading to a simpler model that generalizes better to unseen data.\n",
    "\n",
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "A8. **Limitations of regularized linear models**:\n",
    "- May not perform well if the underlying relationship between predictors and the outcome is nonlinear.\n",
    "- Choosing the appropriate regularization parameter (\\(\\lambda\\)) can be challenging.\n",
    "- Lasso may discard important variables if \\(\\lambda\\) is too high.\n",
    "- Ridge may include irrelevant variables if \\(\\lambda\\) is too low.\n",
    "\n",
    "**Not Always Best Choice**:\n",
    "- When the relationship between predictors and the outcome is complex and nonlinear.\n",
    "- In such cases, other models like decision trees, random forests, or neural networks may be more suitable.\n",
    "\n",
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "A9. \n",
    "- RMSE and MAE measure different aspects of model performance.\n",
    "- RMSE penalizes larger errors more than MAE, so a lower RMSE indicates fewer large errors.\n",
    "\n",
    "**Limitations**:\n",
    "- Without comparing both metrics for both models, it's difficult to choose definitively.\n",
    "- Model B has a lower MAE, indicating better average performance, but Model A's RMSE suggests fewer large errors.\n",
    "\n",
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "A10. \n",
    "- It depends on the context and goals.\n",
    "  - Ridge (Model A) is preferred if all predictors are potentially relevant.\n",
    "  - Lasso (Model B) is preferred if feature selection is needed.\n",
    "\n",
    "**Trade-offs or Limitations**:\n",
    "- Ridge regularization may include all predictors, potentially leading to a less interpretable model.\n",
    "- Lasso may discard relevant predictors if the regularization parameter is too high, can lead to biased estimates for large ùúÜ.\n",
    "- Choosing the appropriate regularization parameter for each method is crucial.\n",
    "- Ridge doesn't perform feature selection, which can be a disadvantage in high-dimensional settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
