{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: What is Bayes' Theorem?\n",
    "\n",
    "A1. Bayes' Theorem is a fundamental theorem in probability theory that describes how to update the probability of a hypothesis based on new evidence. It is widely used in various fields, including statistics, machine learning, and decision-making.\n",
    "\n",
    "### Q2: What is the formula for Bayes' Theorem?\n",
    "\n",
    "A2. The formula for Bayes' Theorem is:\n",
    "\n",
    "$$\n",
    "P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-  P(H|E) is the posterior probability: the probability of hypothesis H given the evidence  E.\n",
    "-  P(E|H) is the likelihood: the probability of the evidence E given that the hypothesis H is true.\n",
    "-  P(H) is the prior probability: the initial probability of the hypothesis H before seeing the evidence E.\n",
    "-  P(E) is the marginal likelihood: the probability of observing the evidence E.\n",
    "\n",
    "### Q3: How is Bayes' Theorem used in practice?\n",
    "\n",
    "A3. Bayes' Theorem is used in various practical applications, including:\n",
    "- **Spam Filtering:** To calculate the probability that an email is spam based on the presence of certain words.\n",
    "- **Medical Diagnosis:** To update the probability of a disease given the presence of certain symptoms.\n",
    "- **Machine Learning:** In classification algorithms like Naive Bayes to predict the class of new instances based on prior observations.\n",
    "\n",
    "### Q4: What is the relationship between Bayes' Theorem and Conditional Probability?\n",
    "\n",
    "A4. Bayes' Theorem is essentially an application of conditional probability. It provides a way to calculate the probability of one event given the probability of another event. Specifically, it allows us to reverse conditional probabilities, going from P(A|B) to P(B|A), and incorporates prior knowledge (prior probability) into the calculation.\n",
    "\n",
    "### Q5: How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "A5. The choice of Naive Bayes classifier depends on the nature of the features in the dataset:\n",
    "- **Gaussian Naive Bayes:** Use when the features are continuous and follow a Gaussian (normal) distribution.\n",
    "- **Multinomial Naive Bayes:** Use when the features represent counts or frequencies (e.g., word counts in text classification).\n",
    "- **Bernoulli Naive Bayes:** Use when the features are binary (e.g., presence or absence of a feature).\n",
    "\n",
    "### Q6: You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "Class |X1=1| X1=2| X1=3| X2=1| X2=2| X2=3| X2=4|\n",
    "------|----|-----|-----|-----|-----|-----|-----|\n",
    "A     |   3|    3|    4|    4|    3|    3|    3|\n",
    "B     |   2|    2|    1|    2|    2|    2|    3|\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?\n",
    "\n",
    "A6. Given the dataset with features  X1 and X2 and classes  A and B, we want to classify a new instance with X1 = 3 and X2 = 4 using Naive Bayes.\n",
    "\n",
    "#### Frequencies:\n",
    "- Class A:\n",
    "  -  X1=3 : 4 occurrences\n",
    "  - x2=4 : 3 occurrences\n",
    "- Class B:\n",
    "  - X1=3 : 1 occurrence\n",
    "  - X2=4 : 3 occurrences\n",
    "\n",
    "#### Step-by-Step Calculation:\n",
    "- **Prior Probabilities (Assumed equal):**\n",
    "  \n",
    "  P(A) = P(B) = 0.5\n",
    "  \n",
    "\n",
    "- **Likelihoods:**\n",
    "  - $$ P(X1=3 | A) = \\frac{4}{10} = 0.4 $$\n",
    "  - $$ P(X2=4 | A) = \\frac{3}{10} = 0.3 $$\n",
    "  - $$ P(X1=3 | B) = \\frac{1}{7} \\approx 0.143 $$\n",
    "  - $$ P(X2=4 | B) = \\frac{3}{7} \\approx 0.429 $$\n",
    "\n",
    "- **Posterior Probabilities:**\n",
    "  - For Class A:\n",
    "   $$\n",
    "    P(A | X1=3, X2=4) \\propto P(X1=3 | A) \\times P(X2=4 | A) \\times P(A)\n",
    "   $$\n",
    "    $$\n",
    "    P(A | X1=3, X2=4) \\propto 0.4 \\times 0.3 \\times 0.5 = 0.06\n",
    "   $$\n",
    "  - For Class B:\n",
    "    $$\n",
    "    P(B | X1=3, X2=4) \\propto P(X1=3 | B) \\times P(X2=4 | B) \\times P(B)\n",
    "    $$\n",
    "    $$\n",
    "    P(B | X1=3, X2=4) \\propto 0.143 \\times 0.429 \\times 0.5 \\approx 0.0307\n",
    "   $$\n",
    "\n",
    "#### Conclusion:\n",
    "Since  P(A | X1=3, X2=4) = 0.06 is greater than  P(B | X1=3, X2=4) = 0.0307 , Naive Bayes would predict the new instance to belong to **Class A**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
