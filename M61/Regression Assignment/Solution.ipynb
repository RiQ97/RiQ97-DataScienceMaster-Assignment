{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "A1. **Simple Linear Regression**:\n",
    "- Involves one independent variable (predictor) and one dependent variable (outcome).\n",
    "- The relationship between the variables is modeled with a straight line: $$ ( y = b_0 + b_1x ) $$\n",
    "- Example: Predicting a person's weight (y) based on their height (x).\n",
    "\n",
    "**Multiple Linear Regression**:\n",
    "- Involves multiple independent variables (predictors) and one dependent variable (outcome).\n",
    "- The relationship is modeled with a linear equation involving multiple terms: $$ y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n $$\n",
    "- Example: Predicting a house's price (y) based on its size (x1), location (x2), and age (x3).\n",
    "\n",
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "A2.The assumptions of linear regression are:\n",
    "1. **Linearity**: The relationship between independent and dependent variables is linear.\n",
    "   - Check: Scatter plots of predicted vs. actual values.\n",
    "2. **Independence**: Observations are independent of each other.\n",
    "   - Check: Durbin-Watson test for autocorrelation.\n",
    "3. **Homoscedasticity**: Constant variance of residuals.\n",
    "   - Check: Plot residuals vs. fitted values.\n",
    "4. **Normality**: Residuals are normally distributed.\n",
    "   - Check: Q-Q plot of residuals or Shapiro-Wilk test.\n",
    "5. **No Multicollinearity**: Independent variables are not highly correlated.\n",
    "   - Check: Variance Inflation Factor (VIF) or correlation matrix.\n",
    "\n",
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "A3. - **Intercept (b0)**: The predicted value of the dependent variable when all independent variables are zero.\n",
    "- **Slope (b1)**: The change in the dependent variable for a one-unit change in the independent variable.\n",
    "\n",
    "Example:\n",
    "- Model: $$ y = b_0 + b_1x $$\n",
    "- Scenario: Predicting a person's weight (y) based on their height (x).\n",
    "- Interpretation: If the intercept ( b_0 = 50 ) and the slope ( b_1 = 0.5 ), then:\n",
    "  - Intercept: When height is 0, the predicted weight is 50 \n",
    "  - Slope: For each additional unit of height, the weight increases by 0.5 units.\n",
    "\n",
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "A4. **Gradient Descent**:\n",
    "- An optimization algorithm to minimize the cost function by iteratively moving towards the steepest descent direction.\n",
    "- Used to find the best-fit line in regression by minimizing the sum of squared errors.\n",
    "\n",
    "Steps:\n",
    "1. Initialize the parameters (coefficients) randomly.\n",
    "2. Compute the gradient (partial derivatives) of the cost function with respect to the parameters.\n",
    "3. Update the parameters by moving in the opposite direction of the gradient.\n",
    "4. Repeat steps 2 and 3 until convergence (when changes in cost function are negligible).\n",
    "\n",
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "A5. **Multiple Linear Regression**:\n",
    "- Involves multiple independent variables (predictors) to predict one dependent variable.\n",
    "- Model: $$ y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n $$\n",
    "- Captures the effect of each predictor on the dependent variable while controlling for other predictors.\n",
    "\n",
    "1. Simple linear regression has only one predictor, whereas multiple linear regression has more than one predictor.\n",
    "2. Multiple linear regression can account for the combined effect of several factors on the outcome.\n",
    "\n",
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "A6. **Multicollinearity** occurs when two or more predictors are highly correlated, making it difficult to isolate their individual effects on the dependent variable.\n",
    "- This leads to unstable coefficient estimates and inflated standard errors.\n",
    "\n",
    "**Detection**:\n",
    "- Variance Inflation Factor (VIF): VIF > 10 indicates high multicollinearity.\n",
    "- Correlation Matrix: High correlation coefficients between predictors.\n",
    "\n",
    "**Addressing**:\n",
    "- Remove one of the correlated predictors.\n",
    "- Combine correlated variables into a single predictor (e.g., principal component analysis).\n",
    "- Ridge regression: A regularization technique that can handle multicollinearity.\n",
    "\n",
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "A7. **Polynomial Regression**:\n",
    "- A form of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial.\n",
    "- Model: $$ y = b_0 + b_1x + b_2x^2 + ... + b_nx^n $$\n",
    "\n",
    "- Linear regression models a straight-line relationship, while polynomial regression models a curvilinear relationship.\n",
    "- Polynomial regression can capture more complex patterns in the data by adding higher-order terms.\n",
    "\n",
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "A8. **Advantages**:\n",
    "- Can model nonlinear relationships.\n",
    "- More flexible than linear regression for capturing complex patterns.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Risk of overfitting, especially with higher-degree polynomials.\n",
    "- Interpretation of coefficients becomes difficult.\n",
    "- Sensitive to outliers.\n",
    "\n",
    "**Situations to Use Polynomial Regression**:\n",
    "- When the relationship between the predictors and the outcome is not linear.\n",
    "- When a visual inspection of the data suggests a curved pattern.\n",
    "- When a simple linear model does not provide a good fit to the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
