{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
    "\n",
    "Design a pipeline that includes the following steps:\n",
    "1. Use an automated feature selection method to identify the important features in the dataset.\n",
    "2. Create a numerical pipeline that includes the following steps\".\n",
    "3. Impute the missing values in the numerical columns using the mean of the column values.\n",
    "4. Scale the numerical columns using standardization.\n",
    "5. Create a categorical pipeline that includes the following steps\"\n",
    "6. Impute the missing values in the categorical columns using the most frequent value of the column.\n",
    "7. One-hot encode the categorical columns.\n",
    "8. Combine the numerical and categorical pipelines using a ColumnTransformer.\n",
    "9. Use a Random Forest Classifier to build the final model.\n",
    "10. Evaluate the accuracy of the model on the test dataset.\n",
    "\n",
    "Note:- Your solution should include code snippets for each step of the pipeline, and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline.\n",
    "\n",
    "### Q1. Designing a Machine Learning Pipeline\n",
    "\n",
    "**Problem Statement:**\n",
    "You're tasked with building a machine learning pipeline to handle a dataset containing both numerical and categorical features. The dataset has missing values and some highly correlated features. The pipeline needs to automate feature engineering, handle missing values, and finally train a Random Forest Classifier.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Automated Feature Selection**: Identify the most important features using an automated method.\n",
    "2. **Numerical Pipeline**:\n",
    "   - Impute missing values in numerical columns using the mean.\n",
    "   - Scale numerical columns using standardization.\n",
    "3. **Categorical Pipeline**:\n",
    "   - Impute missing values in categorical columns using the most frequent value.\n",
    "   - One-hot encode categorical columns.\n",
    "4. **Combine Pipelines**: Use `ColumnTransformer` to combine the numerical and categorical pipelines.\n",
    "5. **Modeling**: Use a Random Forest Classifier.\n",
    "6. **Evaluation**: Assess the model's accuracy on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Loading dataset (assuming it's loaded into a DataFrame 'df')\n",
    "df = pd.read_csv('My_Data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# 1. Automated Feature Selection\n",
    "feature_selector = SelectKBest(score_func=f_classif, k='all')\n",
    "\n",
    "# 2-4. Numerical Pipeline\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', StandardScaler())  # Standardize the numerical features\n",
    "])\n",
    "\n",
    "# 5-7. Categorical Pipeline\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode the categorical features\n",
    "])\n",
    "\n",
    "# 8. Combine Pipelines\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_pipeline, numerical_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# 9. Final Pipeline including the Random Forest Classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', feature_selector),  # Select important features\n",
    "    ('classifier', RandomForestClassifier(random_state=42))  # Use Random Forest Classifier\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 10. Predict on the test set and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation:**\n",
    "- **Feature Selection**: `SelectKBest` uses the ANOVA F-value to identify the most important features.\n",
    "- **Numerical Pipeline**: Handles missing values by imputing the mean and standardizes the numerical features.\n",
    "- **Categorical Pipeline**: Handles missing values by imputing the most frequent value and applies one-hot encoding to the categorical features.\n",
    "- **Preprocessing**: Combines the numerical and categorical pipelines into a single preprocessing step.\n",
    "- **Modeling**: The Random Forest Classifier is applied to the preprocessed data.\n",
    "- **Evaluation**: The accuracy score is computed on the test dataset to evaluate the modelâ€™s performance.\n",
    "\n",
    "**Interpretation of Results:**\n",
    "If the accuracy is satisfactory, the pipeline is performing well. If not, consider tuning hyperparameters or experimenting with different imputation and scaling techniques.\n",
    "\n",
    "**Possible Improvements:**\n",
    "- **Hyperparameter Tuning**: Use `GridSearchCV` or `RandomizedSearchCV` to fine-tune the model.\n",
    "- **Additional Feature Engineering**: Create interaction features or use PCA for dimensionality reduction.\n",
    "- **Outlier Handling**: Consider implementing techniques to detect and mitigate the impact of outliers.\n",
    "\n",
    "\n",
    "\n",
    "### Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy.\n",
    "\n",
    "A2. We need to build a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, and then combine their predictions using a Voting Classifier.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Load the Iris dataset**.\n",
    "2. **Create individual pipelines** for Random Forest and Logistic Regression.\n",
    "3. **Combine the models** using a Voting Classifier.\n",
    "4. **Train the pipeline** and evaluate its accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_pipeline),\n",
    "    ('lr', lr_pipeline)\n",
    "], voting='hard')\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "accuracy = voting_classifier.score(X_test, y_test)\n",
    "print(f'Voting Classifier Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation:**\n",
    "- **Random Forest Pipeline**: Contains a Random Forest Classifier.\n",
    "- **Logistic Regression Pipeline**: Contains a Logistic Regression Classifier.\n",
    "- **Voting Classifier**: Combines the predictions from both classifiers. `voting='hard'` means the final prediction is based on majority voting.\n",
    "- **Evaluation**: The accuracy of the Voting Classifier is computed on the test dataset.\n",
    "\n",
    "**Interpretation of Results:**\n",
    "If the Voting Classifier's accuracy is high, it suggests that combining predictions from multiple models is beneficial. If not, you might need to consider alternative models or improve the preprocessing steps.\n",
    "\n",
    "**Possible Improvements:**\n",
    "- **Soft Voting**: Experiment with `voting='soft'` to average predicted probabilities instead of hard majority voting.\n",
    "- **Additional Models**: Include more diverse classifiers like SVM or KNN in the Voting Classifier.\n",
    "- **Feature Engineering**: Enhance feature engineering to boost model performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
